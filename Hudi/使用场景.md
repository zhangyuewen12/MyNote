## Near Real-Time Ingestion[#](https://hudi.apache.org/docs/use_cases#near-real-time-ingestion)

Hudi offers some great benefits across ingestion of all kinds. Hudi helps **enforces a minimum file size on DFS**. This helps solve the ["small files problem"](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) for HDFS and Cloud Stores alike, significantly improving query performance. Hudi adds the much needed ability to atomically commit new data, shielding queries from ever seeing partial writes and helping ingestion recover gracefully from failures.

> 1. 解决HDFS小文件问题，提高查询效率。
> 2. 对于新增数据提供原子性操作。
> 3. 查询时屏蔽一部分的写入数据，可以从失败的作业中重启。

Ingesting data from OLTP sources like (event logs, databases, external sources) into a [Data Lake](http://martinfowler.com/bliki/DataLake) is a common problem, that is unfortunately solved in a piecemeal fashion, using a medley of ingestion tools. This "raw data" layer of the data lake often forms the bedrock on which more value is created.

For RDBMS ingestion, Hudi provides **faster loads via Upserts**, as opposed costly & inefficient bulk loads. It's very common to use a change capture solution like [Debezium](http://debezium.io/) or [Kafka Connect](https://docs.confluent.io/platform/current/connect/index) or [Sqoop Incremental Import](https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide#_incremental_imports) and apply them to an equivalent Hudi table on DFS. For NoSQL datastores like [Cassandra](http://cassandra.apache.org/) / [Voldemort](http://www.project-voldemort.com/voldemort/) / [HBase](https://hbase.apache.org/), even moderately big installations store billions of rows. It goes without saying that **full bulk loads are simply infeasible** and more efficient approaches are needed if ingestion is to keep up with the typically high update volumes.

Even for immutable data sources like [Kafka](https://kafka.apache.org/), there is often a need to de-duplicate the incoming events against what's stored on DFS. Hudi achieves this by [employing indexes](http://hudi.apache.org/blog/hudi-indexing-mechanisms/) of different kinds, quickly and efficiently.

> 1. 对于关系型数据库管理系统，通过Upsert操作提供更高效的Load。
> 2. 使用CDC工具，然后把捕获的数据变更写到Hudi上对应的表。
> 3. 对于Hbase这种Nosql数据库，完全批量不可行。数据摄取速度跟不上数据更新速度
> 4. Kafka需要根据文件系统上的数据去重，Hudi可以有效解决。

All of this is seamlessly achieved by the Hudi DeltaStreamer tool, which is maintained in tight integration with rest of the code and we are always trying to add more capture sources, to make this easier for the users. The tool also has a continuous mode, where it can self-manage clustering/compaction asynchronously, without blocking ingestion, significantly improving data freshness.

## Data Deletion[#](https://hudi.apache.org/docs/use_cases#data-deletion)

Hudi also offers ability to delete the data stored in the data lake, and more so provides efficient ways of dealing with large write amplification, resulting from random deletes based on user_id (or any secondary key), by way of the `Merge On Read` table types. Hudi's elegant log based concurrency control, ensures that the ingestion/writing can continue happening,as a background compaction job amortizes the cost of rewriting data/enforcing deletes.

Hudi also unlocks special capabilities like data clustering, which allow users to optimize the data layout for deletions. Specifically, users can cluster older event log data based on user_id, such that, queries that evaluate candidates for data deletion can do so, while more recent partitions are optimized for query performance and clustered on say timestamp.

> 