# 第1章 MapReduce概述

## 1.1 MapReduce定义

MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。

MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。

## 1.2 MapReduce优缺点

### 1.2.1 优点

**1****）MapReduce****易于编程**

它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。

**2****）良好的扩展性**

当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。

**3****）高容错性**

MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。

**4****）适合PB****级以上海量数据的离线处理**

可以实现上千台服务器集群并发工作，提供数据处理能力。

### 1.2.2 缺点

**1****）不擅长实时计算**

MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。

**2****）不擅长流式计算**

流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。

**3****）不擅长DAG****（有向无环图）计算**

多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。

## 1.3 MapReduce核心思想

<img src="/Users/zyw/Library/Application Support/typora-user-images/image-20221007102048263.png" alt="image-20221007102048263" style="zoom:50%;" />

（1）分布式的运算程序往往需要分成至少2个阶段。

（2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。

（3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。

（4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。

总结：分析WordCount数据流走向深入理解MapReduce核心思想。





### 

# 二、MapReduce作业运行机制

**MapRecude究竟是如何执行的，从map到shuffle，再到reduce的这一套完整的计算过程是如何调度的呢？这就是MapReduce的作业运行机制。**

  对于一个MapReduce作业，有两种方法来提交使其运行，**一个是Job对象的waitForCompletion()方法**，用于提交以前没有提交过的作业，并等待它的完成；**还有一个是Job对象的submit()方法**，这个方法调用封装了大量的处理细节。

  在整个MapReduce作业的运行过程中，有5个独立的实体：

- 客户端。client负责提交MapReduce作业。
- YARN资源管理器。它负责协调集群上计算机资源的分配。
- YARN节点管理器。它负责启动和监视集群中机器上的计算容器（Container）。
- MapReduce的application master。它负责协调运行MapReduce作业的所有任务，它和MapReduce任务都在容器中运行，这些容器由资源管理器分配并且由节点管理器进行管理。
- 分布式文件系统（一般是HDFS）。它主要用于与其他实体之间共享作业文件

<img src="/Users/zyw/Library/Application Support/typora-user-images/image-20221008161625003.png" alt="image-20221008161625003" style="zoom:50%;" />

## 1 作业的提交

Job的submit()的方法会创建一个内部的JobSummitter实例，并且调用其submitJobInternal()方法提交作业（图中的步骤1），然后通过waitForCompletion()每秒轮询作业的进度，如果发现自上次报告后有改变，就把进度报告到控制台。最后作业完成后，如果成功就显示作业计数器，如果失败，则将错误信息输出到控制台。

  具体来说，整个作业的提交过程如下所示：

1. 向资源管理器请求一个新的应用ID，也就是MapReduce的作业ID（步骤2）。

2. 检查作业的输出说明。例如，如果没有指定输出目录或者输出目录已经存在，作业就不提交，返回错误。

3. 计算作业的输入分片。如果分片无法计算，比如输入路径不存在，作业就不提交，返回错误。

4. 将运行作业所需要的资源（包括作业JAR文件、配置文件、计算到的输入分片）复制到一个以作业ID命名的共享文件系统中（步骤3）。

5. 通过调用资源管理器的submitApplication()方法提交作业（步骤4）。



## 2 作业初始化

  资源管理器接收到调用它的submitApplication()方法的消息之后，将请求传递给YARN的调度器，通过调度器给开启一个容器，然后资源管理器ResourceManger在节点管理器NodeManager的管理下，在该容器中启动一个application master进程。（步骤5、6）

application master的主类为MRAppMaster。由于MRA将接收来自任务的进度和完成报告，因此MRA对作业的初始化是通过创建多个薄记对象以保持对作业进度的跟踪来完成的（步骤7）。

接着MRA接收来自共享文件系统的，在客户端计算的输入分片（步骤7）。然后对每一个分片创建一个map任务对象以及由mapreduce.job.reduces属性确定的多个reduce任务对象。任务ID在此时分配。

mr application master决定如何运行MapReduce作业，如果作业很小，则将在与MRA相同的一个JVM中运行任务，默认情况下，小作业定义为少于10个mapper且只有一个reducer且输入大小小于一个HDFS块的作业。

最后，在任务运行之前，application master 调用setupjob()方法设置OutputCommiter。FileOutputCommiter为默认值，表示将建立作业的最终输出目录及任务输出的临时工作空间。

1. 资源管理器通过节点管理器来启动MR application master，MRA主要用来记录任务运行状态。
2. MRA 接收HDFS的输入分片，为每一个分片建立一个Map任务，以及创建多个Reduce任务。
3. 根据任务大小，判断是否申请额外资源。

## 3.作业分配

1. 如果任务作业不是小作业，则application master向资源管理器请求容器（步骤8）。在请求容器时，Map任务请求的优先级高于Reduce任务，因为Map任务必须首先完成，直到有5%的map任务已经完成时，为reduce任务发起的容器请求才会发出。这个请求也会为每个任务指定需要的内存和CPU数，默认情况下每个任务都会分配到1024MB的内存和一个虚拟的内核。
2. reduce任务能够在集群中的任意位置运行，但是map任务有着数据本地化的局限。map任务输入数据分片有三种情况：数据本地化(data local)、机架本地化(rack local)、从别的机架读取。数据本地化即输入分片和任务在同一节点，这种情况可以直接读取本地数据没有网络IO影响；机架本地化是指分片和任务不在同一节点，但是在统一机架中，该情况会有网络IO影响，但是由于在同一机架因此传输速度相对较快；第三种情况，分片和任务既不在同一节点也不在同一机架。
3. 作业分配也会为map和reduce任务分配内存和CPU资源。

## 4.任务执行

一旦资源管理器的调度器为任务分配了一个特定节点上的容器，application master就可以通过与该节点上的节点管理器来通信，进而启动容器执行任务（步骤9a和9b）。

任务由主类为YarnChild的一个java应用程序执行，在运行之前，首先将任务需要的资源本地化（步骤10），包括作业的配置、JAR文件和所有来自分布式缓存的文件。然后开始运行（步骤11）。

YarnChild在指定的JVM（task JVM）中运行，因此用户自定义的map或者reduce函数中的任何问题不会影响节点管理器(NodeManger)。

除了执行相应的map和reduce计算逻辑外，每个任务还可以执行搭建和提交的动作，和任务本身在同一个JVM运行，这两个动作由作业的OutputCommiter确定，对于默认的文件作业，提交可以将任务输出由临时位置搬到最终位置。

<img src="/Users/zyw/Library/Application Support/typora-user-images/image-20221010151805918.png" alt="image-20221010151805918" style="zoom:30%;" />



Streaming任务使用标准输入和输出流与进程进行通信。在任务执行过程中，Java进程都会把输入键-值对传给外部的进程，后者通过用户定义的map、reduce函数来执行它并把输出键-值对传给Java进行。从节点管理器的角度，将像其子进程自己在运行map或reduce代码一样。

## 5. 进度和状态的更新

  MapReduce作业是长时间运行的批量作业，运行时间为几个小时是很正常的，所以在作业执行期间，用户需要得到一些反馈信息。每个作业和它的任务都有一个状态，包括：作业或任务的状态（运行中、成功、失败）、map和reduce的进度、作业计数器的值、状态消息或描述。

  所谓进度，就是任务完成的百分比。对于Map任务，就是已处理的的输入所占的比例；而对于reduce任务比较复杂，和reduce端的三个阶段相对应，如果任务已经处理了reducer输入数据的一般，那么进度是5/6，因为已经完成了copy和merge（每个占1/3），且又完成了reduce的1/2，所以总进度是5/6。

  除了进度，任务还有很多计数器，可以用于对任务运行过程中的各个事件进行统计。

  当map任务和reduce任务执行时，子进程和自己的父application master通过umbilical接口通信，每隔3秒，任务通过这个接口向自己的application master报告进度和状态，然后由application master形成一个汇聚视图。

  而对于整个作业来说，客户端每秒轮询一次application master以获得最新状态，或者，客户端也可以通过Job的getStatus()方法获得一个JobStatus的实例，它包含了作业相关的状态信息。



## 6. 作业的完成

  当application master收到作业最后一个任务已完成的通知后，就知道作业已经完成，便把作业状态设置为“成功”，然后，下一次Job轮询时，就知道了任务已经完成，就可以从waitForCompletion方法返回，相应的统计信息和计数值输出到控制台。

  最后，application master和任务容器会清理该作业的工作状态，比如删除一些中间输出，OutputCommiter中的一个commitJob方法被调用，将作业信息存档，以便日后查询。

  至此，一个完整的MapReduce作业就被成功的调度和执行成功了。



# 三、Shuffle与排序

<img src="/Users/zyw/Library/Application Support/typora-user-images/image-20221008162126087.png" alt="image-20221008162126087" style="zoom:50%;" />

MapReduce确保每个reducer的输入都按键排序。系统执行排序的过程——将map输出作为输入传给reducer——称为shuffle。shuffle属于不断被优化和改进的代码库的一部分，从许多方面来看，shuffle是MapReduce的“心脏”，是奇迹发生的地方。事实上，shuffle这个说法并不准确。因为在某些语境中，它只代表reduce任务获取map输出的这部分过程。在这里，我们将其理解为从map产生输出到reduce的消化输入的整个过程。

## 1.map端

　　map函数开始产生输出时，并不是简单地将它写到磁盘。这个过程更复杂，它利用缓冲的方式写到内存缓冲区，并出于效率的考虑进行**预排序**。map的输出结果是由collector处理的，所以map端的shuffle过程包含在collect函数对map输出结果的处理过程中。

　　每个map任务都有一个环形内存缓冲区，用于存储任务的输出，默认情况下，缓冲区的大小为100MB，此值可以通过改变io.sort.mb属性来调整。一旦缓冲内容达到阀值(io.sort.spill.percent，默认为0.80，或80%)，一个后台线程便开始把内容溢写(spill)到磁盘中。在写磁盘过程中，map输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘将按轮询方式写到mapred.local.dir属性指定的作业特定子目录中的目录中。

​	在写入磁盘之前，线程首先根据数据最终要传的reducer把数据划分成相应的分区（partition）。在每个分区中，后台线程按键进行分区内排序（快速排序），**如果有一个combiner函数，这个函数会在排序后的输出上执行。**

每次内存缓存区达到溢出阈值，就会新建一个溢出文件（split file），因此在map任务写完其最后一个输出记录之后，会有几个溢出文件。在任务完成之前，溢出文件被合并成一个已分区且已排序的输出文件（归并排序）。

combiner使得map的输出结果更紧凑，减少磁盘写入和传输的数据量。慎用，可能会对结果产生错误的结果；如果存在combiner阶段；



1. **内存预排序。默认每个map任务有100m内存进行预排序，超过阈值，会把内容写到磁盘。算法-快速排序算法。**
2. **分区预排序。在写入磁盘之前，线程首先根据数据最终要传的reducer把数据划分成相应的分区（partition）。在每个分区中，后台线程按键进行分区内排序。算法-归并排序。**

3.  **combiner,使得map的输出结果更紧凑，减少磁盘写入和传输的数据量。慎用，可能会对结果产生错误的结果；如果存在combiner阶段；**
4. **一个Map任务会产生多个spill文件，在Map任务完成前，所有的spill文件将会归并排序为一个索引文件和数据文件。当spill文件归并完成后，Map将删除所有的临时文件，并告知TaskTracker任务已完成。**

## 2.reduce端

1. 复制阶段：Reduce端通过HTTP获取Map端的数据，只要有一个map任务完成，Reduce任务就开始复制它的输出。JobTracker知道Map输出与TaskTracker的映射关系，Reduce端有一个线程间歇地向JobTracker询问Map输出的地址，直到把所有的数据都获取到。

2. 排序阶段，又称合并阶段。将多个已经排序的文件合并成一个文件。Merge有三种形式：内存到内存，内存到磁盘，磁盘到磁盘。

  此过程顺序比较插入排序算法，可能都不叫算法。只是对多个已排序文件合并成一个文件。



# 四、数据倾斜问题

执行任务的时候，任务进度长时间维持在99%，这种现象叫数据倾斜。

数据倾斜是经常存在的，因为有少量的Reduce任务分配到的数据量和其他Reduce差异过大，导致大部分Reduce都已完成任务，但少量Reduce任务还没完成的情况。

解决数据倾斜的问题，可通过设置 "set hive.optimize.skewjion=true",并调整hive.skewjoin.key的大小。hive.skewjoin.key是指Reduce端接收多少个Key即认为数据是倾斜的，并自动分发到多个Reduce。





下面列举了一些常见的导致数据倾斜的场景。
场景 1 ： 当一个大表和一个小表 join 时， 如果小表的 key 较集中，将会引起大表中的数据被分发到一个或者少数几个 Reducer 任务中，导致数据分布不均匀。
场景 2： 在 group by 时，如果分组的维度太少，维度的值分布不均匀，将导致数据分布不 均匀。
场景 3： 当大表与大表关联时，在关联的条件字段中，其中一个表的空值、 null 值过多， 将导致数据分布不均匀。；



解决方案主要分两种：

1 调整参数

```
1 . 调节参数
可以通过修改 hive.map.aggr 和 hive.groupby.skewindata 参数同时配置为 true，在 Mapper 端 进行聚合操作，当发生数据倾斜时进行负载均衡。 所生成的查询计划会有两个 MR 任务。 在第 一个 MR 任务中， Mapper 阶段的输出结果集合会被随机分布到 Reducer 阶段中， 每个 Reducer 都进行部分聚合操作，并输出结果。这样处理的结果是相同的 Key 可以被分发到不同的 Reducer 中，从而达到负载均衡的目的。在第二个 MR 任务中， Mapper 根据第一个 MR 任务预处理后的 数据结果再按照 key输出给 Reducer，这个过程可以保证相同的 key被分布到同一个 Reducer 中 。 经过这两轮 MR 任务最后完成最终的聚合操作。相关的参数设置如下

1. join 数据倾斜。解决数据倾斜的问题，可通过设置 "set hive.optimize.skewjion=true",并调整hive.skewjoin.key的大小。hive.skewjoin.key是指Reduce端接收多少个Key即认为数据是倾斜的，并自动分发到多个Reduce。

2. Group by 数据倾斜。可通过设置“set hive.groupby.skewindata=true”,生成的任务计划会有两个MR任务，第一个job的Map输出结果会随机分布到Redcue中，每个Reduce会进行部分聚合，并输出结果。这样的处理会使相同的group by key 可能被分到不同的Reduce中，从而达到负载均衡。第二个Job会再根据预处理的结果按照Group by key 分发到Reduce中完成最终的聚合操作。
```



2 优化SQL语句。

```sh
使用 mapjoin：让小的维度表（建议在 20000 条记录以下〉先写入内存，并按顺序扫描 大表完成 join。这种方式比较适用于大表和小表的 join。
空值优化： 可以将空值的 key 变成一个字符串加上随机数，把倾斜的数据分布到不同的Reducer中 。 也可以对空值进行单独处理，然后再和其他非空值的计算结果进行合 并。
group by 优化： 采用 sum（）结合 group by 的方式替换 count(distinct）来完成计算。
```



  **上所述，解决数据倾斜问题就是要将 Mapper 阶段的输出数据更均匀地分布到 Reducer 中，可以通改变 job 的步骤、处理 key 值等方式来实现。数据倾斜多数是由于开发人员疏忽引 起的，问题本身并不是很复杂。**