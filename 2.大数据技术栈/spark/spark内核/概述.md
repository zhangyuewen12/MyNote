Overriew
https://github.com/JerryLead/SparkInternals/blob/master/EnglishVersion/1-Overview.md
This chapter aims to answer the following questions:  

Q1: After a successful deployment, what are the running services in each node?
在成功部署系统后，在每个节点上，启动了哪些服务？
Q2: How is the Spark application created and executed?
Spark应用怎么被创建和执行？

# 一、Deployment Diagram 部署图示
![avatar](https://github.com/JerryLead/SparkInternals/blob/master/PNGfigures/deploy.png?raw=true)  
从部署图中可以看出(standalone versioni)：
* 一个spark集群中有一个master节点和多个Worker节点，与hadoop中的Master和slavers类型。
* Master 节点上常驻 Master 守护进程，负责管理全部的 Worker 节点。
* Worker 节点上常驻 Worker 守护进程，负责与 Master 节点通信并管理 executors。
* Driver 官方解释是 “The process running the main() function of the application and creating the SparkContext”。Application 就是用户自己写的 Spark 程序（driver program），比如 WordCount.scala。如果 driver program 在 Master 上运行，比如在 Master 上运行
```
./bin/run-example SparkPi 10
```
那么 SparkPi 就是 Master 上的 Driver。如果是 YARN 集群，那么 Driver 可能被调度到 Worker 节点上运行（比如上图中的 Worker Node 2）。另外，如果直接在自己的 PC 上运行 driver program，比如在 Eclipse 中运行 driver program，使用
```
scala
val sc = new SparkContext("spark://master:7077", "AppName")
``` 
去连接 master 的话，driver 就在自己的 PC 上，但是不推荐这样的方式，因为 PC 和 Workers 可能不在一个局域网，driver 和 executor 之间的通信会很慢
* 每个worker管理一个或多个ExecutorBackend process。每个executorBackend 启动和管理者一个Executor实例。
* 每个application有一个或多个executors。在一个executor中的tasks属于同一个应用。
* 在 Standalone 版本中，ExecutorBackend 被实例化成 CoarseGrainedExecutorBackend 进程。
* worker 通过一个ExecutorRunner对象管理每个CoarseGrainedExecutorBackend进程。

# 一个spark应用
假设使用下面的命令在master中启动应用。
```
Usage: GroupByTest [numMappers] [numKVPairs] [valSize] [numReducers]
bin/run-example GroupByTest 100 10000 1000 36
```
程序代码：
```
package org.apache.spark.examples

import java.util.Random

import org.apache.spark.{SparkConf, SparkContext}

/**
 * Usage: GroupByTest [numMappers] [numKVPairs] [KeySize] [numReducers]
 */
object GroupByTest {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("GroupBy Test")
    var numMappers = if (args.length > 0) args(0).toInt else 2
    var numKVPairs = if (args.length > 1) args(1).toInt else 1000
    var valSize = if (args.length > 2) args(2).toInt else 1000
    var numReducers = if (args.length > 3) args(3).toInt else numMappers

    val sc = new SparkContext(sparkConf)

    val pairs1 = sc.parallelize(0 until numMappers, numMappers).flatMap { p =>
      val ranGen = new Random
      var arr1 = new Array[(Int, Array[Byte])](numKVPairs)
      for (i <- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr)
      }
      arr1
    }.cache()
    // Enforce that everything has been calculated and in cache
    pairs1.count()

    println(pairs1.groupByKey(numReducers).count())

    sc.stop()
  }
}
```